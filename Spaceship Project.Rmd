---
title: "Spaceship Titannic Project"
output: html_document
groupmember: Group 8
date: "2023-07-20"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F)
```

# Project Overview

The *Spaceship Titanic* was an interstellar passenger liner launched a month ago. With **almost 8,700** passengers on board, the vessel set out on its maiden voyage transporting emigrants from our solar system to three newly habitable exoplanets orbiting nearby stars.

While rounding route to its first destination---the torrid 55 Cancri E---the unwary *Spaceship Titanic* collided with a spacetime anomaly hidden within a dust cloud. **Sadly**, it met a similar fate as its namesake from 1000 years before. Though the ship stayed intact, almost **half of the passengers were transported** to an alternate dimension!

![](https://storage.googleapis.com/kaggle-media/competitions/Spaceship%20Titanic/joel-filipe-QwoNAhbmLLo-unsplash.jpg)

\newpage

## Mission

Our task is to **predict** whether a passenger was transported to an alternate dimension during the *Spaceship Titanic*'s collision with the space-time anomaly.

\newpage

## Dataset Description

**train.csv** - Personal records for about two-thirds (\~8700) of the passengers, to be used as training data.

[**1 Target - "Transported" -** Whether the passenger was transported to another dimension.]{.underline}

**13 Variables**

-   `PassengerId` - A unique Id for each passenger.

-   `HomePlanet` - The planet the passenger departed from.

-   `CryoSleep` - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.

-   `Cabin` - The cabin number where the passenger is staying. Takes the form `deck/num/side`, where `side` can be either `P` for *Port* or `S` for *Starboard*.

-   `Destination` - The planet the passenger will be debarking to.

-   `Age` - The age of the passenger.

-   `VIP` - Whether the passenger has paid for special VIP service during the voyage.

-   `RoomService`, `FoodCourt`, `ShoppingMall`, `Spa`, `VRDeck` - Amount the passenger has billed at each of the *Spaceship Titanic*'s many luxury amenities.

-   `Name` - The first and last names of the passenger.

\newpage

# Data Engineering

Data Preparation and Engineering:

-   The dataset was loaded and preprocessed, handling missing values and converting data types as needed.

-   The 'Cabin' feature was split into separate columns, providing additional insights into passengers' cabin arrangements.

-   Certain features were converted into factors to prepare the data for modeling.

```{r}
library(tidyverse)
library(xgboost)
library(caret)
library(ggplot2)
library(pROC)
library(naniar)
```

```{r}
# convert character variables to factor variables 
# specify blank cells and recoginize them as missing values

setwd("/Users/sam/Desktop/Summer/STA380/spaceship_titanic/data")
train <- read.csv("train.csv", header = TRUE, stringsAsFactors = T, na.strings = "")
train$CryoSleep <- as.integer(as.logical(train$CryoSleep))
train$VIP <- as.integer(as.logical(train$VIP))
train$Transported<- as.integer(as.logical(train$Transported))
```

```{r}
dim(train)
colSums(is.na(train))
```

```{r}
vis_miss(train)
```

\newpage

## Missing Value Imputation

**Be Careful!!!** *CryoSleep*

We build our little decision tree on how to fill the missing values.

-   Are you asleep? ---\> **"I don't know"** ---\> delete the whole row

-   Are you asleep? ---\> **"Yes"** ---\> fill the blank in amenity columns with 0

-   Are you asleep? ---\> **"No"**---\> fill the blank in amenity columns with mean

-   Other Missing Values: fill with most frequency or mean value of non-missing values in that column

```{r}
# If NaN in CryoSleep, delete the whole row
train_clean <- train[complete.cases(train$CryoSleep), ]
```

```{r}
# If NaN in activities while asleep, fill 0
train_clean$RoomService <- ifelse(train_clean$CryoSleep == 1 & is.na(train_clean$RoomService), 0, train_clean$RoomService)
train_clean$FoodCourt <- ifelse(train_clean$CryoSleep == 1 & is.na(train_clean$FoodCourt), 0, train_clean$FoodCourt)
train_clean$ShoppingMall <- ifelse(train_clean$CryoSleep == 1 & is.na(train_clean$ShoppingMall), 0, train_clean$ShoppingMall)
train_clean$Spa <- ifelse(train_clean$CryoSleep == 1 & is.na(train_clean$Spa), 0, train_clean$Spa)
train_clean$VRDeck<- ifelse(train_clean$CryoSleep == 1 & is.na(train_clean$VRDeck), 0, train_clean$VRDeck)
```

```{r}
# Check no activities while sleeping
condition <- is.na(train_clean$RoomService) | is.na(train_clean$FoodCourt)| 
  is.na(train_clean$ShoppingMall) | is.na(train_clean$Spa) | is.na(train_clean$VRDeck)
num_rows <- sum(train_clean$CryoSleep=='1' & condition == 'TRUE')
num_rows
```

```{r}
# Impute missing value for the train and test data.
# Replace numerical NA with the most frequently occurring non-missing value
train_clean$HomePlanet[is.na(train_clean$HomePlanet)] <- names(sort(table(train_clean$HomePlanet),decreasing = T))[1]
train_clean$CryoSleep[is.na(train_clean$CryoSleep)] <- names(sort(table(train_clean$CryoSleep),decreasing = T))[1]
train_clean$Destination[is.na(train_clean$Destination)] <- names(sort(table(train_clean$Destination),decreasing = T))[1]
train_clean$VIP[is.na(train_clean$VIP)] <- names(sort(table(train_clean$VIP),decreasing = T))[1]
train_clean$Cabin[is.na(train_clean$Cabin)] <- names(sort(table(train_clean$Cabin),decreasing = T))[1]
```

```{r}
# Replace numerical NAs with mean value 
train_clean$Age[is.na(train_clean$Age)] <- mean(train_clean$Age, na.rm=T)
train_clean$RoomService[is.na(train_clean$RoomService)] <- mean(train_clean$RoomService, na.rm=T)
train_clean$FoodCourt[is.na(train_clean$FoodCourt)] <- mean(train_clean$FoodCourt, na.rm=T)
train_clean$ShoppingMall[is.na(train_clean$ShoppingMall)] <-mean(train_clean$ShoppingMall, na.rm=T)
train_clean$Spa[is.na(train_clean$Spa)] <- mean(train_clean$Spa, na.rm=T)
train_clean$VRDeck[is.na(train_clean$VRDeck)] <- mean(train_clean$VRDeck, na.rm=T)
```

## Data Extraction

Since the Cabin column contains 3 different information, we **split the Cabin column into 3 independent columns**. Then, I **converted the categorical data to factor data type** and re-code **them as either 0 or 1** before ignore the meaningless features.

```{r}
train_clean$Deck <- factor(sapply(strsplit(as.character(train_clean$Cabin), "/"), "[", 1))
train_clean$Deck_number <- factor(sapply(strsplit(as.character(train_clean$Cabin), "/"), "[", 2))
train_clean$Side <- factor(sapply(strsplit(as.character(train_clean$Cabin), "/"), "[", 3))
```

```{r}
train_clean$CryoSleep <- as.factor(train_clean$CryoSleep)
train_clean$VIP <- as.factor(train_clean$VIP)
train_clean$Deck <- as.factor(train_clean$Deck)
train_clean$Transported <- as.factor(train_clean$Transported)
```

```{r}
colSums(is.na(train_clean))
dim(train_clean)
```

## EDA

**Explore the insights** of the data to see which features could be helpful for our prediction.

```{r}
ggplot(train_clean, aes(x=CryoSleep, fill=Transported)) + geom_bar(position = "fill") +
  labs(y="Percent") + scale_y_continuous(labels = scales::percent)
```

```{r}
ggplot(train_clean, aes(x=VIP, fill=Transported)) + geom_bar(position = "fill") +
  labs(y="Percent") + scale_y_continuous(labels = scales::percent)

```

```{r}
train_noSleep = filter(train_clean, CryoSleep == 0)
ggplot(data = train_noSleep, mapping = aes(x = Age, y = Transported, color = VIP))+
  geom_boxplot(notch=TRUE,
               outlier.colour="red", 
               outlier.shape = 1,
               outlier.size = 1)+
  coord_flip()
```

```{r}
ggplot(train_clean, aes(x=HomePlanet, fill=Transported)) + geom_bar(position = "fill") +
  labs(y="Percent") + scale_y_continuous(labels = scales::percent)
```

```{r}
ggplot(train_clean, aes(x=Destination, fill=Transported)) + geom_bar(position = "fill") +
  labs(y="Percent") + scale_y_continuous(labels = scales::percent)
```

```{r}
ggplot(train_clean, aes(x=Deck, fill=Transported)) + geom_bar(position = "fill") +
  labs(y="Percent") + scale_y_continuous(labels = scales::percent)
```

```{r}
ggplot(train_clean, aes(x=Side, fill=Transported)) + geom_bar(position = "fill") +
  labs(y="Percent") + scale_y_continuous(labels = scales::percent)
```

```{r}
# delete PassengerId','Name','Cabin' and 'Deck Number"Column
train_clean <- train_clean[,-which(names(train_clean) %in% c('PassengerId','Name','Cabin','Deck_number'))]

train_clean$CryoSleep <- as.factor(train_clean$CryoSleep)
train_clean$VIP <- as.factor(train_clean$VIP)
train_clean$Deck <- as.factor(train_clean$Deck)
train_clean$Transported <- as.factor(train_clean$Transported)
```

```{r}
colSums(is.na(train))
colSums(is.na(test))
```

\newpage

# Model Selection

-   Logistic Regression

-   KNN

-   Decision Tree

-   Random Forest

-   Xtreme Gradient Boost

\newpage

## Logistic Regression

### Steps Performed

1.  Data Splitting : 80% of the sample size

2.  

```{r}
# Data Splitting -----------
## 80% of the sample size
smp_size <- floor(0.8 * nrow(train))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(train_clean)), size = smp_size)

train_split <- train_clean[train_ind, ]
validation_split <- train_clean[-train_ind, ]

```

```{r}

```

## KNN

### Steps Performed

1.  Normalize numerical variables (Age and amounts) using maximum and minimum values

2.  Convert categorical variables to dummy variables

3.  Convert response variable (Transported) to factor

4.  Run recursive feature elimination (RFE) to select the important variables using caret package. Important variables found to be (CryoSleep, Spa, RoomService, VRDeck, FoodCourt)

5.  Perform KNN with the above important variables and for various k-flods cross validation

6.  Selected k-fold as 8 from the results which is having less test error rate

7.  Perform KNN for various k values with 8-fold cross validation.

\newpage

### Performance

plot of error rate vs k:

![](KNN%20Error%20Rate.png)

### Final Model

|                                  |        |
|----------------------------------|--------|
| K value with minimum error rate: | 20     |
| Minimum error rate               | 0.2112 |
| Accuracy                         | 78.88% |
| k-fold CV selected               | 8      |

## Decision Tree

## Random Forest

\newpage

## Xtreme Gradient Boost

XGBoost, a powerful gradient boosting algorithm, was chosen for modeling due to its effectiveness in binary classification tasks.

In this part, we aimed to optimize the performance of an XGBoost model through hyperparameter tuning using k-fold cross-validation.

### Steps Performed

1.  Hyperparameter Tuning and Cross-Validation:

-   XGBoost, a powerful gradient boosting algorithm, was chosen for modeling due to its effectiveness in binary classification tasks.

-   A 5-fold cross-validation strategy was implemented to robustly assess model performance and prevent overfitting.

-   A parameter grid was defined to explore different combinations of hyperparameters, including:

    -   **`eta`**: The learning rate.

    -   **`max_depth`**: The maximum depth of a decision tree.

    -   **`nrounds`**: The number of boosting rounds (iterations).

    -   **`subsample`**: The subsample ratio of training instances for each tree.

```{r,echo = FALSE}
set.seed(42)
train_indices <- createDataPartition(train_clean$Transported, p = 0.8, list = FALSE)
train_data <- train_clean[train_indices, ]
test_data <- train_clean[-train_indices, ]

# Convert character variables to factors in the training data
# Convert the target variable 'y_train' and 'y_test' to numeric
x_train <- data.matrix(train_data[, -which(names(train_data) == "Transported")])
y_train <- as.numeric(as.character(train_data$Transported))

x_test <- data.matrix(test_data[, -which(names(test_data) == "Transported")])
y_test <- as.numeric(as.character(test_data$Transported))
```

```{r}

# Set the seed for reproducibility
set.seed(42)

# Define the number of folds for cross-validation
n_folds <- 5

# Create cross-validation folds
cv_folds <- createFolds(y_train, k = n_folds)

# Define the parameter grid for hyperparameter tuning
param_grid <- expand.grid(
  eta = seq(0.02, 1, 0.02),
  max_depth = seq(5, 10, 1),
  nrounds = seq(100, 600, 100),
  subsample = seq (0.5, 1, 0.1)
)

# Initialize a matrix to store validation accuracies for each parameter combination and fold
val_accuracy <- matrix(NA, nrow = nrow(param_grid), ncol = n_folds)


```

2.  Model Training and Evaluation:

-   The XGBoost model was trained using various hyperparameter combinations obtained from cross-validation.

-   Model performance was evaluated on the validation set for each parameter combination, and the average validation accuracy was used as the criterion for selecting the best hyperparameters.

```{r}
# Perform k-fold cross-validation
for (fold in 1:n_folds) {
  # Get the training and validation data for this fold
  x_train_fold <- x_train[-cv_folds[[fold]], ]
  y_train_fold <- y_train[-cv_folds[[fold]]]
  x_val_fold <- x_train[cv_folds[[fold]], ]
  y_val_fold <- y_train[cv_folds[[fold]]]
  
  # Train the xgboost model with different parameter combinations
  for (i in 1:nrow(param_grid)) {
    # Define xgboost model
    model <- xgboost(data = as.matrix(x_train_fold), label = as.numeric(y_train_fold),
                     eta = param_grid$eta[i], max_depth = param_grid$max_depth[i],
                     nrounds = param_grid$nrounds[i], subsample = param_grid$subsample[i],objective = "binary:logistic",nthread = -1)
    
    # Predict on the validation set
    yhat_val <- predict(model, as.matrix(x_val_fold))
    
    # Convert probabilities to binary predictions
    yhat_val_class <- ifelse(yhat_val > 0.5, 1, 0)
    
    # Calculate validation set accuracy
    val_accuracy[i, fold] <- mean(yhat_val_class == y_val_fold)
  }
}
```

3.  Best Model Selection and Testing:

-   The model with the highest average validation accuracy was selected as the final model.

-   The selected model was trained on the entire training dataset using the best hyperparameters.

-   The model's accuracy was evaluated on the previously unseen test dataset to estimate its performance on new data.

```{r}

# Find the index of the best parameter combination
best_param_index <- which.max(rowMeans(val_accuracy))

# Get the best parameters
best_eta <- param_grid$eta[best_param_index]
best_max_depth <- param_grid$max_depth[best_param_index]
best_nrounds <- param_grid$nrounds[best_param_index]
best_subsample <- param_grid$subsample[best_param_index]

cat("Best Parameters:\n")
cat("eta =", best_eta, "\n")
cat("max_depth =", best_max_depth, "\n")
cat("nrounds =", best_nrounds, "\n")
cat("subsample =", best_subsample, "\n")
```

```{r}
# Use the best parameters to train the final model
final_model <- xgboost(data = as.matrix(x_train), label = y_train,
                       eta = best_eta, max_depth = best_max_depth,
                       nrounds = best_nrounds, subsample = best_subsample,objective = "binary:logistic", nthread = -1)

# Evaluate the final model's performance on the test set
yhat_test <- predict(final_model, as.matrix(x_test))
yhat_test_class <- ifelse(yhat_test > 0.5, 1, 0)
accuracy_test <- mean(yhat_test_class == y_test)
cat("Final Model accuracy on test set:", accuracy_test, "\n")
```

Visualizations:

-   Feature importance was visualized to highlight the most influential variables in predicting passenger transportation.

-   Learning curves were plotted to showcase how the model's accuracy improves with increasing training data size.

-   Residual plots and error distribution plots provided insights into the model's predictive errors.

-   ROC and PR curves were generated to analyze the model's classification performance.

-   A confusion matrix heatmap illustrated the model's performance across different prediction outcomes.

```{r}
# Feature Importance Plot =================================
# Convert Feature to factor and reorder based on Gain
feature_importance <- xgb.importance(feature_names = colnames(x_train), model = final_model)
feature_importance$Feature <- factor(feature_importance$Feature, levels = feature_importance$Feature[order(-feature_importance$Gain)])

# Plot the bar chart with re-ordered levels
ggplot(data = feature_importance, aes(x = Feature, y = Gain)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Feature Importance",
       x = "Feature", y = "Gain") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
# Learning Curves ==========================================
train_sizes <- seq(0.1, 1, 0.1)
train_accuracy <- c()
val_accuracy <- c()

for (size in train_sizes) {
  n_samples <- floor(size * nrow(train_data))
  train_accuracy_fold <- c()
  val_accuracy_fold <- c()
  
  for (fold in 1:n_folds) {
    indices <- sample(1:nrow(train_data), n_samples)
    x_subset <- x_train[indices, ]
    y_subset <- y_train[indices]
    
    # Get the training and validation data for this fold
    x_val_fold <- x_train[cv_folds[[fold]], ]
    y_val_fold <- y_train[cv_folds[[fold]]]
    
    # Train the model on the subset
    model <- xgboost(data = as.matrix(x_subset), label = y_subset,
                     eta = best_eta, max_depth = best_max_depth,
                     nrounds = best_nrounds, subsample = best_subsample)
    
    # Predict on the training and validation sets
    yhat_train <- predict(model, as.matrix(x_subset))
    yhat_val <- predict(model, as.matrix(x_val_fold))
    
    # Convert probabilities to binary predictions
    yhat_train_class <- ifelse(yhat_train > 0.5, 1, 0)
    yhat_val_class <- ifelse(yhat_val > 0.5, 1, 0)
    
    # Calculate training and validation set accuracies for this fold
    train_accuracy_fold <- c(train_accuracy_fold, mean(yhat_train_class == y_subset))
    val_accuracy_fold <- c(val_accuracy_fold, mean(yhat_val_class == y_val_fold))
  }
  
  # Take the average of accuracies from all folds
  train_accuracy <- c(train_accuracy, mean(train_accuracy_fold))
  val_accuracy <- c(val_accuracy, mean(val_accuracy_fold))
}

# Plot Learning Curves
ggplot(data = learning_curves_data, aes(x = Train_Size)) +
  geom_line(aes(y = Training_Accuracy, color = "Training Accuracy")) +
  geom_line(aes(y = Validation_Accuracy, color = "Validation Accuracy")) +
  geom_point(aes(y = Final_Model_Accuracy, color = "Final Model Accuracy"), size = 3) +
  labs(title = "Learning Curves",
       x = "Training Set Size", y = "Accuracy") +
  scale_color_manual(values = c("Training Accuracy" = "blue", "Validation Accuracy" = "red", "Final Model Accuracy" = "green")) +
  theme_minimal()
```

```{r}
# Residual Plot ======================================
residuals <- yhat_test - y_test
plot(y_test, residuals, main = "Residual Plot", xlab = "True Values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
# Error Distribution Plot ===========================
error <- abs(yhat_test - y_test)
hist(error, main = "Error Distribution", xlab = "Absolute Error", col = "green", breaks = 30)
```

```{r}
# ROC Curve and PR Curve ============================
# Train the final model on the entire training data
final_model <- xgboost(data = as.matrix(x_train), label = y_train,
                       eta = best_eta, max_depth = best_max_depth,
                       nrounds = best_nrounds, subsample = best_subsample, objective = "binary:logistic", nthread = -1)

# Predict probabilities on test set
yhat_prob <- predict(final_model, as.matrix(x_test))
```

```{r}
# ROC Curve
roc_curve <- roc(y_test, yhat_prob)
plot(roc_curve, main = "ROC Curve", col = "blue")
lines(x = c(0, 1), y = c(0, 1), col = "gray", lty = 2)
legend("bottomright", legend = paste("AUC =", round(auc(roc_curve), 2)), col = "blue")
```

```{r}
# PR Curve
pr_curve <- prcurve(y_test, yhat_prob)
plot(pr_curve, main = "PR Curve", col = "red")
legend("bottomright", legend = paste("AUC =", round(auc(pr_curve), 2)), col = "red")
```

```{r}
# Confusion Matrix ================================
yhat_test_class <- ifelse(yhat_test > 0.5, 1, 0)
conf_matrix <- table(Actual = y_test, Predicted = yhat_test_class)
print(conf_matrix)

# Convert confusion matrix to data frame
conf_matrix_df <- as.data.frame(conf_matrix)

# Plot confusion matrix heat map
ggplot(conf_matrix_df, aes(x = Predicted, y = Actual, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(low = "white", high = "blue") +
  labs(title = "Confusion Matrix Heat Map", x = "Predicted", y = "Actual") +
  theme_minimal()
```

```{r}
# Precision-Recall Curve ===========================
prc_curve <- pr.curve(scores.class0 = 1 - yhat_prob, weights.class0 = y_test)
plot(prc_curve, main = "Precision-Recall Curve", col = "purple")
legend("bottomright", legend = paste("AUC =", round(auc(prc_curve$curve), 2)), col = "purple")
```

Conclusion: Through systematic data preprocessing, hyperparameter tuning, and cross-validation, we successfully developed an optimized XGBoost model for predicting whether spaceship passengers would be transported to their destination. The selected model demonstrated high accuracy on the test set, indicating its potential for real-world predictions. The visualizations provided valuable insights into the model's performance and feature importance, aiding in the interpretation and understanding of the predictive results.
